
\subsection{First Sketch}

From a selection of the video provided by the user, the best characteristics of the desired object are obtained, using the Shi-Tomasi algorithm. 
%A partir de una selección del video proporcionada por el usuario se obtienen las mejores features del objeto deseado, utilizando el algoritmo de Shi-Tomasi.

Afterwards, through Lucas-Kanade's Optical Flow, the evolution of these features over time is obtained. The center of mass $\mu$ and the center of inertia $\sigma $ are calculated from this cluster of points. The points that lie at a distance greater than $k_{\sigma} \sigma$ with respect to the center of mass are rejected, to then calculate said center again, which is the one that is introduced in the Kalman filter as position measurement.
%Luego, mediante el de Lucas-Kanade de Optical Flow se obtiene la evolución de dichas features en el tiempo. A este cúmulo de puntos se le calcula el centro de masas $\mu$ y el centro de inercia $\sigma$. Se rechazan los puntos de este cúmulo que se encuentren a una distancia mayor a $k_{\sigma}\sigma$ respecto al centro de masas, para luego calcular nuevamente dicho centro, el cual es el que se introduce en el filtro de Kalman como medición de la posición.

The dimension of the state vector of the Kalman filter is four, having both horizontal and vertical position and velocity. The dimension of the observation vector is two, since only the horizontal and vertical position of the center of mass is measured. The matrix $ A $ is defined by the equations that characterize the motion of the object to be tracked, which, for simplicity and both good results, were considered as the kinematic equations of MRU, obtaining thus

%La dimensión del vector de estados del filtro de Kalman es de cuatro, poseyendo la posición y la velocidad tanto horizontal como vertical. La dimensión del vector de observación es de dos, dado que solamente se mide la posición horizontal y vertical del centro de masas. La matriz $A$ queda definida por las ecuaciones que caracterizan el movimiento del objeto a seguir que, por simplicidad, se consideraron las ecuaciones cinemáticas de MRU, quedando entonces

\begin{equation}
\begin{pmatrix} x_{k-1} + dt\cdot v_{x_{k-1}} \\y_{k-1} + dt\cdot v_{y_{k-1}} \\ v_{x_{k-1}}  \\ v_{y_{k-1}} \end{pmatrix} =\begin{pmatrix}
1 & 0 & dt  & 0\\
0 & 1  & 0 & dt\\
0 & 0  & 1  & 0\\
0 & 0  & 0  & 1 
\end{pmatrix} \cdot \begin{pmatrix} x_{k-1}\\y_{k-1} \\ v_{x_{k-1}}  \\ v_{y_{k-1}} \end{pmatrix}
\end{equation}
where, 
\begin{equation}
A = 
\begin{pmatrix}
1 & 0 & dt  & 0\\
0 & 1  & 0 & dt\\
0 & 0  & 1  & 0\\
0 & 0  & 0  & 1 
\end{pmatrix}
\end{equation}

Then, the observation or measurement transition matrix is defined by the transformation of the observation space to the real space of the filter state vector. As the observables to be measured are part of the real space of the state vector and are only the positions, the matrix is defined in a simple way.
%Then, the observation or measurement transition matrix is ​​defined by the transformation of the observation space to the real space of the filter state vector. As the observables to be measured are part of the real space of the state vector and are only the positions, the matrix is ​​defined in a simple way

%Luego, se tiene que la matriz de observación o de transición de medición queda definida por la transformación del espacio de observación al espacio real del vector de estados del filtro. Como los observables a medir forman parte del espacio real del vector de estados y son solo las posiciones, la matriz queda definida de manera simple
\begin{equation}
\begin{pmatrix} z_{x_{k}} \\ z_{y_{k}} \end{pmatrix}=\begin{pmatrix}
1 & 0 & 0  & 0\\
0 & 1  & 0 & 0  \end{pmatrix} \cdot  \begin{pmatrix} x_k \\ y_k \\ v_{x_{k}} \\ v_{y_{k}}
\end{pmatrix}
\end{equation}
where,
\begin{equation}
H = 
\begin{pmatrix}
1 & 0 & 0  & 0\\
0 & 1  & 0 & 0 
\end{pmatrix}
\end{equation}

The process covariance matrix $Q$ and measurement covariance matrix $R$ are square by definition, of dimension four and two respectively. Furthermore, they are of the form $ \mathbb{I} \cdot \xi $ since independence of the random variables is assumed for simplicity, leaving

%Las matrices de covarianza del proceso $Q$ y de medición $R$ son cuadradas por definición, de dimensión cuatro y dos respectivamente. Además, son la forma $\mathbb{I}\cdot \xi$ dado que se asume independencia de las variables aleatorias, quedando

\begin{equation}
Q = 
\begin{pmatrix}
\xi_Q & 0 & 0  & 0\\
0 & \xi_Q  & 0 & 0\\
0 & 0  & \xi_Q  & 0\\
0 & 0  & 0  & \xi_Q 
\end{pmatrix}
\end{equation}

\begin{equation}
R = 
\begin{pmatrix}
\xi_R & 0 \\
0 & \xi_R 
\end{pmatrix}
\end{equation}

Where $\xi_Q$ and $\xi_R$ are the variances of the process and measurement respectively.

%Siendo $\xi_Q$ y $\xi_R$ las varianzas del proceso y de medición respectivamente.

Finally, $\hat{x}_{k | k}$ and $P_{k | k}$ take the form
%Finalmente, $\hat{x}_{k|k}$ y $P_{k|k}$ toman la forma

\begin{equation}
\hat{x}_{k|k} = 
\begin{pmatrix}
\hat{x}\\
\hat{y}\\
\hat{v_x}\\
\hat{v_y}
\end{pmatrix}
\end{equation}

\begin{equation}
P_{k|k} = 
\begin{pmatrix}
\gamma_{x} & 0 & 0  & 0\\
0 & \gamma_{y}  & 0 & 0\\
0 & 0  & \gamma_{v_x}  & 0\\
0 & 0  & 0  & \gamma_{v_y} 
\end{pmatrix}
\end{equation}

$\Gamma_{i}$ being the variance of each state variable.
%Siendo $\gamma_{i}$ la varianza de cada variable de estado. Cabe notar que el modelo utilizado asume independencia entre las variables aleatorias por simplicidad.

The algorithm explained until now has big problems. For example, given two objects $A$ and $B$, where $A$ is the object to follow:

%Este algoritmo explicado, cuenta con grandes problemas. Dados dos objetos $A$ y $B$, donde $A$ es el objeto a seguir:

\begin{itemize}
\item Since the Shi-Tomasi algorithm tends to place features on well-defined edges, and these are usually the outer edges of an object, if the object to track $A$ passes in front of $B$ with well-defined edges, the Features of the Optical Flow algorithm tend to stick to the edges of $B$ no matter how long it is still.

%\item Como el algoritmo de Shi-Tomasi tiende a colocar features en bordes bien definidos, y estos suelen ser los bordes externos de un objeto, si el objeto a seguir $A$ pasa por delante de un objeto $B$ con bordes bien definidos, las features del algoritmo de Optical Flow tienden a adosarse a los bordes del objeto $B$ por más que este esté quieto.
\item If the object $A$ passes now behind the immobile object $B$, all the features on the object to be followed are attached to the object $B$. This poses a much bigger problem than the previous one, given that the estimation of the Kalman filter cannot be used reliably, because when the features stay still and attached to the outer edges of $B$, the position is still being measured as if there were no problem and those measurements are continually being delivered to the Kalman filter. This ocassionates that the kalman estimate will remain attached to the $B$ object instead of continue through the estimated trajectory of $A$.

%\item Si el objeto $A$ pasase por detrás del objeto $B$ inmóvil, todas las features sobre el objeto a seguir quedan adosadas al objeto $B$. Esto plantea un problema mucho más grande que el anterior, dado que no se puede utilizar la estimación del filtro de Kalman de manera confiable, debido a que cuando los features se queden quietos adosados en $B$, se sigue midiendo de todas formas y se sigue entregando esas mediciones al filtro de Kalman, por lo que la estimación de este se adosa también al objeto $B$.
\end{itemize}




























\subsection{Second Sketch}

\subsubsection{HSV-Based Color Masking}

The proposed improvement to overcome the previous problems consists of adding a color filter to differentiate between $A$ and $B$ by their color. The implementation of this color filter begins the moment the user selects the area to follow. The median of the color of the chosen area is calculated, and that color is transformed from RGB to HSV format, or Hue-Saturation-Value, posing an easier way of defining a mask this way.
%La mejora propuesta para sortear estos problemas consiste en agregar un filtro de color, para diferenciar a $A$ y a $B$ por su color. La implementación de este filtro de color comienza en el momento en el que el usuario selecciona el área a seguir. Se calcula la mediana del color de la zona elegida, y ese color es transformado de formato RGB a formato HSV, o Hue-Saturation-Value, dado que es más sencillo definir una máscara de este modo. 

Given $x$ and $y$ as the coordinates of the pixels of the image, $\vec{R_k}$, $\vec{G_k}$ and $\vec{B_k}$ as the vectors containing the pixels of the initial selection, $median(\vec{R_k}, \vec{G_k}, \vec{B_k}) = (m_R, m_G, m_B)$ as the vector that defines the median of the color of the selection, $\mathbb{HSV}$ as the transformation operator between RGB and HSV, and $\mathbb{HSV}(m_R, m_G, m_B) = (m_H, m_S, m_V)$ as the vector that defines the messdian of the color of the selection in the HSV space, the color mask is then defined as

%Dado $x$ e $y$ las coordenadas de los píxeles de la imagen, $\vec{R_k}$, $\vec{G_k}$ y $\vec{B_k}$ los vectores de píxeles de la selección, $mediana(\vec{R_k}, \vec{G_k}, \vec{B_k}) = (m_R, m_G, m_B)$ el vector que define la mediana del color de la selección, $\mathbb{HSV}$ el operador de transformación del espacio de coordenadas RGB a HSV, y $\mathbb{HSV}(m_R, m_G, m_B) = (m_H, m_S, m_V)$ el vector de la mediana del color de la selección en el espacio HSV, queda entonces definida la máscara de color como

\begin{equation}
f(x, y) = (f_h(x, y), f_s(x, y), f_v(x, y))
\end{equation}

\begin{equation}
f_{u}(x,y) = \left\{
\begin{array}{ll}


      1 & -\Lambda_u < u_{x, y} - m_u < \Lambda_u \\
      
      
      
      0 & \text{otherwise} \\
      
      
\end{array} 
\right.
\end{equation}

where $\Lambda $ is the limit for each $u$ property, these being Hue, Saturation or Value. Once the mask $f(x,y)$ is obtained, the AND operation is carried out between the mask and the original image, obtaining a null intensity in the pixels of the image that do not possess a color close to the calculated median. This can be seen in the Figures (\ref{fig:col1}) and (\ref{fig:col2}).

%donde $\Lambda$ es el límite para cada propiedad $u$, siendo estas Hue, Saturation o Value. Una vez obtenida la máscara $f(x, y)$ se realiza la operación AND entre la máscara y la imagen original, logrando obtener una intensidad nula en los píxeles de la imagen que no posean un color cercano a la mediana calculada como se puede ver en las Figuras (\ref{fig:col1}) y (\ref{fig:col2}).

\begin{figure}[H]
\centering
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Imagenes/color1.png}
		\caption{Screencap before applying the color mask to follow the blue cap.}
		\label{fig:col1}
	\end{subfigure}
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Imagenes/color2.png}
		\caption{Screencap after applying the color mask. It is with this new image instead of the old one that the algorithm operates.}
		\label{fig:col2}
	\end{subfigure}
	\caption{Comparison between before and after applying the color mask.}
	\label{fig:color_comp}
\end{figure}
This color filtering not only has great synergy with the Shi-Tomasi feature obtaining method, since this frame has much better quality edges, but it also has great synergy with the Optical Flow method, due to the fact that as fewer edges are seen in the modified image, the probability that the features are all contained within or on the edges of the object to be tracked increases notably.
%Este filtrado de color no solo posee una gran sinergia con el método de obtención de features de Shi-Tomasi, dado que este frame tiene bordes de mucha mejor calidad, sino que también posee una gran sinergia con el método de Optical Flow, debido a que como se ven menos bordes en la imagen modificada, aumenta notablemente la probabilidad de que los features queden contenidos todos dentro, o sobre los bordes del objeto a seguir.

\begin{figure}[h]
\centering
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Imagenes/Optical1.png}
		\caption{Normal operation of the prototype program where an orange circle is used to indicate the estimated position.}
		\label{fig:optical1}
	\end{subfigure}
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Imagenes/Optical2.png}
		\caption{Actual vision of the algoritm after color masking and indicating the optical flow points. It can be seen how color masking sinergically enhances the reliability of the Shi-Tomasi and Optical Flow algorithms.}
		\label{fig:optical2}
	\end{subfigure}
	\caption{Aftermath of selecting an object both in the back-end and front-end of the algorithm.}
	\label{fig:optical12}
\end{figure}

\subsubsection{Search for Lost Object}

However, the Kalman filter cannot estimate the position of the object indefinitely. This is why when the object is lost, that is, no more features are detected, the algorithm enters another phase. In this phase, a search for the object is performed in each frame of the video based on the estimation of the Kalman filter and the original size of the selected area. The height and width of the search area are increased by a constant factor for each frame in which the search fails. This search is based on applying the Shi-Tomasi algorithm on the mentioned area.

%Sin embargo, el filtro de Kalman no puede estimar la posición del objeto indefinidamente. Es por esto que cuando el objeto se pierde, es decir, no se logran detectar más features, se entra en otra fase del programa. En esta fase, se realiza una búsqueda del objeto en cada frame del video a partir de la estimación del filtro de Kalman y el tamaño original del área seleccionada. La altura y el ancho del área de búsqueda se incrementa por un factor constante por cada frame en la que la búsqueda falle. Esta búsqueda se basa en aplicar el algoritmo de Shi-Tomasi sobre el área mencionada.

\subsubsection{Periodic Correction of the Characteristic Points}

Finally, to reinforce the tracking algorithm, a periodic recalculation of the features was also implemented with the Shi-Tomasi method. We start from the mean of the cluster of features, removing the outliers, recalculating the mean again and using this position to apply the Shi-Tomasi algorithm, in the same way as it is done on the first selection provided by the user. All of this allows to even further raise the reliability of the tracking. However, there are still more problems.

%Finalmente, para reforzar el algoritmo de seguimiento, se implemento además una recalculación periódica de las features con el método de Shi-Tomasi. Se parte de la media del cúmulo de features, removiendo los outliers, volviendo a calcular nuevamente la media y utilizar esta posición para aplicar el algoritmo de Shi-Tomasi, de la misma manera que se realiza sobre la primera selección proporcionada por el usuario. Todo esto permite subsanar varios de los problemas planteados anteriormente, sin embargo, existen aún otros problemas.

\subsubsection{Fail Scenarios}

Some cases where the algorithm is found to fail are:
%Algunos casos donde puede fallar hasta ahora el Tracker son:
\begin{itemize}
\item The change in trajectory of an object that is occluded.
%\item El cambio de trayectoria de un objeto una vez que desaparece, provocando una falla en la predicción.
\item Interaction between two objects of similar color.
%\item Interacción entre el objeto trackeado y un objeto de características similares, causando un error de detección
\item Changes on object color, for example a change in illumination.
%\item Cambios bruscos de iluminación o en color del objeto.
\end{itemize}







Having improved the reliability since the last sketch considerably, let again be the objects $A$ and $B$, where $A$ is the object to follow and propose the same situations as before: 
%Volviendo al caso de los objetos $A$ y $B$:

\begin{itemize}
\item If $A$ and $B$ are of different colors, with the color filter, the algorithm never detects that there is a $ B $ object, so tracking it is impossible. Consequently, only $A$ is followed and there is no problem of $A$ passing in front of $B$.

%\item Si $A$ y $B$ son de colores distintos, con el filtro de color, el algoritmo nunca detecta que existe un objeto $B$ por lo que seguirlo es imposible. Por consecuencia, se sigue únicamente a $A$ y no se tiene el problema de $A$ pasando por frente de $B$.

\item If $A$ and $B$ are of different colors, and $A$ now passes behind $B$, the features are now not attached to the $B$ object, but the algorithm sees that the area of $A$ gets smaller as it is hidden by $B$ instead. As the features have no edges to follow, they disappear, so the Kalman filter recieves no new measurements and it correctly estimates the position of $A$ and long as it has not undergone any acceleration while it was hidden.

%\item Si $A$ y $B$ son de colores distintos, y $A$, el objeto a seguir, pasa por detrás de $B$, ahora no se adosan las features al objeto $B$, sino que el algoritmo ve que el área de $A$ se achica a medida que este queda oculto por $B$. Como los features no tienen bordes para seguir, estos desaparecen, por lo que se deja de entregar mediciones al filtro de Kalman y este estima correctamente la posición de $A$ mientras que este no haya sufrido aceleración alguna mientras estaba oculto. 
\end{itemize}























\subsection{Final Design}

In the last iteration of the design process, low computational cost was sacrificed for greater robustness in the algorithm, significantly reducing the probabilities of:

%En la última iteración del proceso de diseño se sacrificó bajo costo computacional por una mayor robustez en el algoritmo, reduciendo significativamente las probabilidades de:
\begin{itemize}
\item Type I error: the object to follow cannot be found, even though it is visible.
%\item Error tipo I: no se encuentra el objeto a seguir por más que sea visible.
\item Type II error: an object is detected even though the object to be tracked is not on the screen.
%\item Error tipo II: se detecta un objeto por más que el objeto a seguir no se encuentre en pantalla.
\item Type III error: another object is detected even though the object to follow is visible.
%\item Error tipo III: se detecta a otro objeto por más que el objeto a seguir este visible.
\end{itemize}
through the use of \textit{Matched Filters}. In addition, it was possible to obtain a greater robustness against changes in lighting using an alternative masking algorithm based on the hue distribution of the object to follow. Finally, with the purpose of presenting this work as the final project of the Digital Analysis of Signals and Systems subject, a graphical interface was developed where one can upload a video or use a camera in real time, follow several objects at the same time, modify any parameter of any of the algorithms used in real time, and even an optimization method was developed for the parameters of the masking algorithm. In Figure (\ref{gui}) one can see a screenshot of the developed graphical interface and in Figure (\ref{fig:Flowchart}) one can visualize the entire flow diagram of the developed tracking algorithm. Figure(\ref{fig:class}) provides a view of how the mentioned graphical interface is organized.


%mediante el uso de \textit{Matched Filters}. Además, se logró obtener una gran robustez frente a cambios en la iluminación utilizando un algoritmo alternativo de enmascaramiento basado en la distribución de hue del objeto a seguir. Por último, se desarrolló una interfaz gráfica donde se puede cargar un video o bien utilizar una cámara en tiempo real, seguir varios objetos a la vez, modificar cualquier parámetro de cualquiera de los algoritmos utilizados en tiempo real e incluso se desarolló un método de optimización automático de los parámetros del algoritmo de enmascaramiento. En la Figura (\ref{gui}) se puede observar una captura de la interfaz gráfica desarrollada y en la Figura (\ref{fig:Flowchart}) se puede observar el diagrama de flujo entero del algoritmo de seguimiento desarrollado. La Figura (\ref{fig:class}) aporta una visión de cómo se organiza el programa mencionado.


\begin{figure*}
	\centering
	\includegraphics[width=0.9\textwidth]{../flowchart.jpg}
	\caption{Flow-chart of the algorithm.}
	\label{fig:Flowchart}
\end{figure*}

\begin{figure*}
	\centering
	\includegraphics[width=0.9\textwidth]{Imagenes/gui.png}
	\caption{Screencap of the developed graphical interface.}
	\label{gui}
\end{figure*}

\begin{figure*}
\centering
	\includegraphics[width=0.8\textwidth]{Imagenes/camshift_mask1.png}
	\caption{First example of the Hue histogram-based masking algorithm. In this example one can see the efficiency archieved in the masking process. Even if there are several objects in the scene with a color very similiar to the object of interest, they are excluded bby the masking process. In addition, and more importantly, it can be noted how the masking algorithm does not exclude the area of the book under shadow. This shows the robustness to changes in illumination of the algorithm.}
	%ejemplo del algoritmo de enmascaramiento basado en histogramas de hue. En este ejemplo se puede notar la eficiencia conseguida en la máscara, debido a que por más que se encuentren varios objetos en escena de un color muy similar al objeto de interés, estos son exluídos por la máscara. Además, y de mayor importancia, se puede notar como el algoritmo de enmascaramiento no excluye la zona del libro bajo sombra. Esto muestra la gran robustez frente a cambios en la iluminación del algoritmo.}
	\label{fig:c_mask1}
\end{figure*}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{Imagenes/Classes.jpg}
	\caption{Class diagram of the aforementioned graphical interface for the presentation of this work.}
	\label{fig:class}
\end{figure}




















\subsubsection{CIE-Lab Based Masking}

El diseño anterior poseía un algoritmo de enmascaramiento basado en el espacio de color HSV. Si bien este funcionaba correctamente, este espacio de color fue reemplazado por CIE-Lab debido a que este posee un solo parámetro que caracteriza la iluminación de un objeto, es decir, con un valor nulo de L se obtiene el negro, mientras que con un valor máximo de L se obtiene blanco, pasando entre medio de esos dos valores por el color representado por a y b. En el espacio HSV, si se utiliza el máximo valor de la componente value, no necesariamente se obtiene blanco, sino que depende del valor de saturación elegido, mientras que si value es nulo, siempre se obtiene negro. Poseer un solo parámetro que correctamente caracterice la iluminación de un objeto brinda una mejor optimización de los valores de semiamplitudes L-a-b utilizados en el enmascaramiento. 

La generación de la máscara se realiza de la misma forma que en el diseño anterior, con el único agregado de un suavizamiento de la máscara computada previo a la utilización de esta para el filtrado de la imagen. Esto se utiliza debido a que generalmente los bordes de un objeto poseen mayor o menor iluminación dependiendo de dónde este la fuente de luz más cercana. Estas diferencias en la iluminación pueden ocasionar que un objeto de similar color al de interés sea vez correctamente excluído de la imagen a excepción de sus bordes, lo cual genera problemas al pasar por delante o por detrás del objeto a seguir por causas detalladas anteriormente.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.45\textwidth]{Imagenes/medianblur.png}
	\caption{Ejemplo del uso del suavizado de la máscara de color. La segunda foto es la máscara previo al suavizado y la tercer foto es posterior al suavizado.}
	\label{fig:medianblur}
\end{figure}

Sin embargo, si bien la utilización del espacio de color CIE-Lab subsanó varios problemas, el enmascaramiento de la imagen tomando en cuenta solamente uno de los varios colores del objeto a seguir falla cuando se encuentra en pantalla un objeto del mismo color que el utilizado para la máscara. Para lidiar con este problema, se decidió desarrollar un método de enmascaramiento que tenga en cuenta todos los colores del objeto a seguir.





















\subsubsection{Enmascaramiento Basado en Histogramas de Hue}
Para este método de enmascaramiento, el espacio de color HSV fue utilizado debido a que un solo parámetro es el encargado de denotar el color, lo cual genera que la distribución de color de un objeto sea una variable aleatoria unidimensional y no bidimensional como lo sería en el espacio de color CIE-Lab. Aclarado esto, los objetos pueden adquirir perfiles de color complejos, sus colores pueden estar distribuidos entorno a un solo valor de hue dominante, o pueden tener más de uno en su composición, obteniendo distribuciones unimodales o multimodales, respectivamente. 
\begin{figure}[H]
	\centering
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{Imagenes/camshift_hist_bimodal.PNG}
		\caption{Distribución de color bimodal.}
		\label{fig:bimodal}
	\end{subfigure}
	
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{Imagenes/camshift_hist_unimodal.PNG}
		\caption{Distribución de color unimodal.}
		\label{fig:unimodal}
	\end{subfigure}

\end{figure}
Para obtener una máscara que podamos utilizar debemos establecer un límite para así dejar pasar solamente aquellos pixeles cuyo hue tenga una alta probabilidad de pertenecer al objeto seleccionado. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.32\textwidth]{Imagenes/mascara_bimodal.PNG}
	\caption{Máscara obtenida luego de quedarnos con los pixeles más probables.}
	\label{fig:noisy_lpf}
\end{figure}
Debemos notar en la Figura (\ref{fig:noisy_lpf}) que se pueden observar claramente las letras y el fondo de la tapa del libro. Como se puede apreciar, el histograma del ejemplo presenta una distribución bimodal la cual ha sido exitosamente capturada por el método de filtrado.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.32\textwidth]{Imagenes/mascara_bimodal_lpf.PNG}
	\caption{Aplicamos el equivalente a un filtro pasabajos para disminuir el ruido.}
	\label{fig:lpf_hist}
\end{figure}

Sin embargo, la Figura (\ref{fig:noisy_lpf}) presenta ciertos pixeles que pueden interferir con las features calculadas mediante Shi-Tomasi. Es por eso que utilizamos la operación de suavizado para eliminar el ruido. Esto trae acarreado otro beneficio, el interior de la mascara se vuelve más uniforme y permisivo para los pixeles que pertenecen al interior del objeto.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{Imagenes/camshift_mask3.png}
	\caption{Comaparación antes y después del filtrado.}
\end{figure}
 

\begin{figure}[H]
\centering
	\includegraphics[width=0.45\textwidth]{Imagenes/camshift_mask2.png}
	\caption{Segundo ejemplo del algoritmo de enmascaramiento basado en camshift. Se observa como el enmascaramiento se realiza a partir de la distribución de colores y no de los colores por separado, dado que por más que los carretes de hilo azules y amarillos posean colores dentro de la selección original (el libro), estos son excluídos por el algoritmo de enmascaramiento. Se editó a posteriori el histograma de colores de la selección sobre la imagen ya filtrada.}
	\label{fig:c_mask2}
\end{figure}

























%\subsubsection{Algoritmo de Optimización de los Parámetros del Algoritmo de Enmascaramiento}
%le cambie el nombre a la seccion por este porque seguramente pueda hacer andar optimize.shgo de scipy con el filtro de LAB que tiene los parametros rikis continuos



























\subsubsection{Algoritmo correlación}
Para el algoritmo de correlación no solo se utiliza el filtro explicado en la Sección \ref{sec:corr}, sino que ademas se trata tanto al frame como al kernel, aplicando una transformación al espacio HSV. La trasformación de RGB a HSV posee un requerimiento computacional significativamente menor a la transformación de RGB a CIE-Lab y por eso se utilizó nuevamente el espacio HSV, dado que este filtrado sería complementario al método de la correlación y no lo más importante en el algoritmo. El filtrado se realizó bajo las siguientes especificaciones: 
            
\begin{align}
MSK = H \  \epsilon \ (0,180 ) \ \ S \  \epsilon \ (50,255) \ \ V \ \epsilon \ (32,255)
\end{align}
Este filtrado del kernel logra eliminar aquellos pixeles de muy baja saturación (grises) y de muy bajo valor (negros), lo cual se comprobó experimentalmente que aumenta considerablemente la precisión en el algoritmo.
Luego, se utiliza el filtro de correlación y se obtiene el punto donde esta es mayor. Se toma una sección alrededor de este punto proporcional a la selección inicial y se compara con el frame original. Si no comparten las características de color que son calculadas por el enmascaramiento de color o distribución de color, este punto de correlación es desechado. Esto se logra aplicando una máscara en la sección del frame de mayor correlación, realizando sobre este nuevamente el cálculo de máxima correlación. Este proceso se realiza unas pocas veces con el propósito de no comprometer la velocidad de procesamiento entre frames. En caso de no encontrar al objeto se considera que hubo un error de trackeo (oclusión del objeto).\\

\begin{figure}[H]
\centering

		\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{Imagenes/corrmask.png}
		\caption{Correlación con máscara ante oclusión del objeto}
		\label{fig:sqdiff2}
	\end{subfigure}
	\begin{subfigure}{.1\textwidth}
		\centering
		\includegraphics[width=1.5\textwidth]{Imagenes/kernelhsv.png}
		\caption{Kernel}
		\label{fig:kernel2}
	\end{subfigure}
	\caption{Aplicación del algoritmo de correlación con máscara.}
	\label{fig:corrtest2}
\end{figure}

Este algoritmo es utilizado en dos escenarios. En el primero se utiliza para la búsqueda de un objeto perdido. En caso de oclusión se utiliza el algoritmo para encontrar nuevamente el objeto, siendo efectivo siempre que no haya objetos que compartan tanto forma y color con el de interés. Por otro lado, en caso de que así sea y el objeto de interés se encuentre oculto, es posible que el algoritmo devuelva un falso positivo, siendo conviene utilizar el alternativo de Shi-Tomasi y medias móviles, dado que el tener información de la posición previa para el tracking resulta fundamental para poder distinguir entre estos.

El otro escenario es utilizar la correlación para el recálculo periódico de features, esto es beneficioso cuando se da la interacción de dos objetos $A$ y $B$ siendo $A$ el de interés y $B$ uno similar. Si $B$ pasa por delante de $A$ quitandole las features, el cálculo periódico por correlación puede detectar que se está siguiendo a $B$ y no a $A$, por lo que corrige este error. Una situación similar a esta no puede ser resuelta por el algoritmo de Shi-Tomasi con medias móviles.



































\subsubsection{Optimización del Código}
Se realizó una optimización del código, quitando redundancias y utilizando únicamente librerías que son corridas en C++ con la finalidad de que el procesamiento en tiempo real sea eficiente. Se optó por un enfoque OOP, esto permitió escalar el proyecto y obtener una mayor modularidad. \\
Además se diseñó una GUI con la capacidad de trackeo multi-objeto, personalización de algoritmos, contando con la opción de variar algoritmos tanto de búsqueda como de recálculo entre medias móviles y correlación, al igual que filtrado por color o camshift. Existen la opción de cambiar las especificaciones de los filtros en tiempo real para obtener el filtro óptimo o que un algoritmo de optimización las calcule automaticamente. También se permiten diversas pantallas de debug, es decir, observar los ditintos filtros a la vez, al igual que una estimación de la posición y velocidad del objeto. Finalmente cuenta con la capacidad de utilizar tanto un video en tiempo real como un video pre-existente.





% Finalmente se presenta un diagrama en bloques del programa, donde se puede apreciar como interaccionan todos %los algoritmos trabajando en armonía.
%\begin{figure}[H]
%\centering
%	\includegraphics[width=0.5\textwidth]{Imagenes/diagram.png}
%	\caption{Diagrama en bloques del programa.}
%	\label{fig:bloques}
%\end{figure}





\subsubsection{Escenarios de fallas}
Si bien se han minimizado las probabilidades de cometer errores de tipo I y tipo II, aún se plantean los siguientes problemas:
\begin{itemize}
%\item En caso de oclusión del objeto de interés $A$ y en la presencia de otro objeto de cualidades similares $B$ el algoritmo podrá dar un falso positivo.
%\item El cambio de trayectoria de un objeto ocluido provocará una falla en la predicción de la trayectoria, si bien a diferencia del diseño anterior, ahora no habrá problemas en retornar al objeto correcto una vez que vuelva a aparecer en pantalla.
\item El algoritmo se encuentra en su situación más frágil al haber un objecto idéntico o de gran similaridad tanto en color como en forma con el que se quiere seguir, debido a que ni el filtro de distribución de color ni el filtro de correlación lograrán distinguir entre ambos. En este caso la única distinción entre ambos será la retención de esquinas características al objeto correcto proporcionado por el algoritmo de Optical Flow. 
\item La correcta elección de parámetros de los algoritmos encargados de la fase de enmascaramiento para cada situación proporcionan una gran robustez frente a cambios en la iluminación. Sin embargo, cuanto más robusto frente a cambios en la iluminación es el algoritmo, mayor será la probabilidad de cometer errores del tipo II (seguir a un objeto incorrecto) debido a que el enmascaramiento tendrá una mayor permisividad. 
\end{itemize}



